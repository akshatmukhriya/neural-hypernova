#!/bin/bash
set -e

# --- ANCHOR PATHS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $1${COLOR_RESET}"; }

function ignite() {
    log "üöÄ Ignite Sequence: Neural Hypernova V1.1.1"

    # 1. Terraform
    cd "$TF_DIR"
    terraform init -reconfigure
    terraform apply -auto-approve

    # 2. Metadata
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    
    # 3. Kubeconfig
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repos
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo update

    # 5. Cilium (Fixed Template Error)
    log "Injecting Cilium 1.16.5 (eBPF Data Plane)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. Karpenter Injection
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait

    # CRITICAL: Wait for Karpenter CRDs and Webhooks to be live
    log "Waiting for Karpenter Webhooks..."
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 7. KubeRay
    log "Deploying KubeRay Operator..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2

    log "Injecting NVIDIA Device Plugin..."
        helm upgrade --install nvidia-device-plugin nvidia/nvidia-device-plugin \
          --namespace kube-system \
          --set-string gfd.enabled=true

    # 8. Manifests
    log "Applying AI Forge Workloads..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "‚úÖ NEURAL HYPERNOVA IS LIVE."
}

function demo() {
    log "üîç Discovering Hypernova Infrastructure..."
    
    # 1. Sync
    cd "$TF_DIR"
    terraform init -reconfigure > /dev/null
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 2. Inject the Dashboard Proxy (The Billion-Dollar Fix)
    log "üåê Deploying Dedicated Dashboard LoadBalancer..."
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb # High-performance NLB
spec:
  type: LoadBalancer
  selector:
    ray.io/node-type: head # Targets the Ray Head pod directly
    ray.io/cluster: hypernova-ray
  ports:
  - name: dashboard
    protocol: TCP
    port: 8265
    targetPort: 8265
EOF

    # 3. Wait for NLB
    log "‚è≥ Waiting for NLB Propagation..."
    local ELB_DNS=""
    for i in {1..12}; do
        ELB_DNS=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        if [ ! -z "$ELB_DNS" ]; then break; fi
        sleep 10
    done

    echo -e "${COLOR_GREEN}----------------------------------------------------------------"
    echo "üöÄ NEURAL HYPERNOVA IS LIVE"
    echo "DASHBOARD: http://$ELB_DNS:8265"
    echo "----------------------------------------------------------------${COLOR_RESET}"
}

function nuke() {
    log "‚ò¢Ô∏è Scorched Earth Protocol..."
    cd "$TF_DIR"
    # Try to get metadata for CLI cleanup before destroying TF
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    
    # 1. Delete K8s Resources
    kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found || true
    
    # 2. Terraform Destroy
    terraform destroy -auto-approve || true
    
    # 3. CLI Clean up for orphaned ENIs
    log "Final Sweep of Security Groups..."
    SGS=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*$CLUSTER_NAME*" --query "SecurityGroups[*].GroupId" --output text)
    for sg in $SGS; do
        aws ec2 delete-security-group --group-id $sg || true
    done
    log "üåë ZERO TRACE."
}

# --- THE COMMAND ROUTER (Ensure 'demo' is listed here!) ---
case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;; # THIS WAS LIKELY MISSING
    *) echo "Usage: $0 {ignite|nuke|demo}" ;;
esac
