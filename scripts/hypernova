#!/bin/bash
set -e

# --- ARCHITECTURAL ANCHORS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

# --- ELITE VISUALS ---
COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RED='\033[0;31m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $(date +'%H:%M:%S') | $1${COLOR_RESET}"; }
function error_log() { echo -e "${COLOR_RED}[HYPERNOVA-ERROR] $1${COLOR_RESET}"; }

# --- THE IGNITE PROTOCOL (Zero to Sovereign) ---
function ignite() {
    log "üöÄ Ignite Sequence: Neural Hypernova V1.1.2 (Industrial Release)"

    # 1. Infrastructure Synthesis (Terraform)
    cd "$TF_DIR"
    log "Synthesizing Infrastructure in $TF_DIR..."
    terraform init -reconfigure
    terraform apply -auto-approve

    # 2. Metadata Extraction
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --query "cluster.resourcesVpcConfig.vpcId" --output text)
    log "Cluster DNA Extracted: $CLUSTER_NAME in $REGION (VPC: $VPC_ID)"

    # 3. Kubeconfig Synchronization
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repository Registry
    log "Registering High-Scale Repositories..."
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
    helm repo update

    # 5. Data Plane Injection (Cilium eBPF)
    log "Injecting Cilium 1.16.5 (Kernel-Level Networking)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. AWS Load Balancer Controller (The Connectivity Bridge)
    log "Deploying AWS Load Balancer Controller with IAM Role..."
    # Get the Role ARN from Terraform
    ROLE_ARN=$(cd "$TF_DIR" && terraform output -raw lb_controller_role_arn)

    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
      --namespace kube-system \
      --set clusterName="$CLUSTER_NAME" \
      --set serviceAccount.create=true \
      --set serviceAccount.name=aws-load-balancer-controller \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ROLE_ARN" \
      --set region="$REGION" \
      --set vpcId="$VPC_ID" \
      --wait

    # 7. Karpenter (Just-In-Time GPU Scaling)
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait
    
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 8. AI Forge Core (KubeRay + NVIDIA)
    log "Deploying AI Orchestration (KubeRay & NVIDIA Plugin)..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2
    helm upgrade --install nvidia-device-plugin nvdp/nvidia-device-plugin \
      --namespace kube-system \
      --set gfd.enabled=true \
      --wait

    # 9. Workload Activation
    log "Applying Neural Hypernova Manifests..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "‚úÖ NEURAL HYPERNOVA IS ARMED AND OPERATIONAL."
}

# --- THE DEMO PROTOCOL (Public Visibility) ---
function demo() {
    log "üîç Discovering Hypernova Context..."
    
    cd "$TF_DIR"
    terraform init -reconfigure > /dev/null
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 1. Create Public Proxy (Avoiding Headless Service Collision)
    log "üåê Provisioning Industrial NLB via AWS LBC..."
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    # Force the health check to hit the dashboard directly
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "8265"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: TCP
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local # Preserves Client IP and ensures local routing
  selector:
    ray.io/node-type: head
  ports:
  - name: dashboard
    protocol: TCP
    port: 8265
    targetPort: 8265
EOF

    # 2. AWS Propagation Wait
    log "‚è≥ Phase 1: Waiting for DNS Assignment..."
    local ELB_DNS=""
    while [ -z "$ELB_DNS" ]; do
        ELB_DNS=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        [ -z "$ELB_DNS" ] && sleep 10
    done
    log "DNS Identity Assigned: $ELB_DNS"

    # 3. Global DNS Resolution Guard
    log "‚è≥ Phase 2: Waiting for Global DNS Propagation (NXDOMAIN Guard)..."
    local RESOLVED="false"
    local ATTEMPTS=0
    while [ "$RESOLVED" != "true" ] && [ $ATTEMPTS -lt 20 ]; do
        if nslookup "$ELB_DNS" > /dev/null 2>&1; then
            RESOLVED="true"
            log "‚úÖ DNS Resolution Verified."
        else
            log "Still propagating... ($((ATTEMPTS*15))s)"
            sleep 15
            ATTEMPTS=$((ATTEMPTS+1))
        fi
    done

    # 4. Connectivity Finalization
    log "‚è≥ Phase 3: NLB Target Warming..."
    sleep 30

    echo -e "${COLOR_GREEN}----------------------------------------------------------------"
    echo "üöÄ NEURAL HYPERNOVA: SPECTATOR MODE ONLINE"
    echo "----------------------------------------------------------------"
    echo "URL: http://$ELB_DNS:8265"
    echo "----------------------------------------------------------------"
    echo "ARCHITECTURAL VALIDATION:"
    echo "1. Entry: AWS NLB v2 (Active)"
    echo "2. Networking: Cilium eBPF (Bypassing Kube-Proxy)"
    echo "3. Compute: Karpenter JIT NodePool (Ready)"
    echo "----------------------------------------------------------------${COLOR_RESET}"
}

# --- THE NUKE PROTOCOL (Total Erasure) ---
function nuke() {
    log "‚ò¢Ô∏è STARTING JANITOR PROTOCOL: DEEP PURGE ENGAGED"

    # 1. Recover Metadata via S3 State
    cd "$TF_DIR"
    log "Refreshing Terraform State..."
    # -upgrade fixes the "Unreadable module directory" error by re-downloading modules
    terraform init -reconfigure -upgrade > /dev/null || true
    
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")

    # 2. Sync Context
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME" 2>/dev/null || true

    # 3. K8s Resource Purge (Triggering AWS cleanup)
    log "Deleting K8s Resources..."
    kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found --timeout=30s || true
    kubectl delete svc hypernova-dashboard-lb --ignore-not-found --timeout=30s || true
    
    # CRITICAL: Wait for AWS to notice the LoadBalancer is gone
    log "‚åõ Waiting 60s for AWS LoadBalancer detachment..."
    sleep 60

    # 4. Terraform Destruction
    log "Executing Terraform Destroy..."
    terraform destroy -auto-approve || true

    # 5. THE JANITOR SWEEP (The Brute Force)
    # If Terraform failed, we manually kill the ENIs and SGs using the VPC ID
    if [ ! -z "$VPC_ID" ]; then
        log "üßπ Hunting Orphaned Resources in VPC: $VPC_ID"
        
        # Kill all available ENIs in this VPC
        ENIS=$(aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text)
        for eni in $ENIS; do
            log "Force-deleting ENI: $eni"
            aws ec2 delete-network-interface --network-interface-id "$eni" || true
        done

        # Kill all Security Groups in this VPC (except default)
        SGS=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query "SecurityGroups[?GroupName!='default'].GroupId" --output text)
        for sg in $SGS; do
            log "Force-deleting SG: $sg"
            aws ec2 delete-security-group --group-id "$sg" || true
        done
    fi

    log "üåë ZERO TRACE REMAINING. ACCOUNT IS CLEAN."
}

# --- ROUTER ---
case "$1" in
    ignite) ignite ;;
    demo) demo ;;
    nuke) nuke ;;
    *) echo "Usage: $0 {ignite|demo|nuke}" ;;
esac