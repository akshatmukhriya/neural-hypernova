#!/bin/bash
set -e

# --- CONFIG ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA] $1${COLOR_RESET}"; }

# --- THE BOOTSTRAP ---
function bootstrap() {
    log "ðŸ“¡ Detecting Cloud Context..."
    ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    BUCKET="hypernova-state-$ACCOUNT_ID"
    REGION="us-east-1"

    if ! aws s3 ls "s3://$BUCKET" > /dev/null 2>&1; then
        log "ðŸ—ï¸ Creating State Foundation: $BUCKET"
        aws s3 mb "s3://$BUCKET" --region $REGION
        aws s3api put-bucket-versioning --bucket "$BUCKET" --versioning-configuration Status=Enabled
    fi
    
    # THE UNICORN FIX: Use a Bash Array for clean argument passing
    INIT_OPTS=(-backend-config="bucket=$BUCKET")
}

# --- THE IGNITE ---
function ignite() {
    bootstrap
    log "ðŸš€ Ignite Sequence: Neural Hypernova V1.2.1"
    cd "$TF_DIR"
    
    # 1. Force Clean Local Context
    rm -rf .terraform .terraform.lock.hcl
    [ -f "backend.tf" ] && rm -f "backend.tf"
    [ -f "outputs.tf" ] && rm -f "outputs.tf"

    log "Initializing Terraform with Persistent State..."
    terraform init "${INIT_OPTS[@]}" -reconfigure

    # --- THE GHOST HUNTER (The Unicorn Fix) ---
    log "ðŸ”Ž Checking for Ghost Infrastructure..."
    if aws eks describe-cluster --name neural-hypernova --region us-east-1 > /dev/null 2>&1; then
        log "âš ï¸ Ghost Cluster Detected! Syncing State..."
        # We attempt to import the cluster into the state so Terraform 'owns' it again
        # This prevents the 'AlreadyExists' error
        terraform import "${INIT_OPTS[@]}" module.eks.aws_eks_cluster.this[0] neural-hypernova || log "Import skipped or failed, proceeding to apply..."
    fi

    log "Executing Infrastructure Synthesis..."
    terraform apply -auto-approve

    # 2. Extract & Warm up
    NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    aws eks update-kubeconfig --region "$REGION" --name "$NAME"

    log "â³ Waiting for EKS Public API to warm up..."
    sleep 30 

    # Operational Layers...
    helm repo add cilium https://helm.cilium.io/
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add ray-project https://ray-project.github.io/kuberay-helm/
    helm repo update

    log "Injecting Core Infrastructure Layers..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 -n kube-system --set kubeProxyReplacement=true --wait
    
    ROLE_ARN=$(terraform output -raw lb_role_arn)
    VPC_ID=$(terraform output -raw vpc_id)
    kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system \
      --set clusterName="$NAME" --set serviceAccount.create=true \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ROLE_ARN" \
      --set region=us-east-1 --set vpcId="$VPC_ID" --wait

    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.1.1 -n karpenter --create-namespace --set settings.clusterName="$NAME" --wait
    helm upgrade --install kuberay-operator ray-project/kuberay-operator --version 1.2.2 -n default
    
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"
    log "âœ… FORGE ONLINE."
}

# --- THE NUKE ---
function nuke() {
    bootstrap
    log "â˜¢ï¸ STARTING SOVEREIGN ERASURE..."
    cd "$TF_DIR"
    
    # 1. Context Sync
    terraform init "${INIT_OPTS[@]}" -reconfigure || true
    CLUSTER_NAME="neural-hypernova"
    REGION="us-east-1"
    
    # 2. Kill the Limbs (Node Groups) - Critical Fix
    log "Dismantling Node Groups..."
    NGS=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region "$REGION" --query "nodegroups" --output text 2>/dev/null || echo "")
    for ng in $NGS; do
        aws eks delete-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name "$ng" --region "$REGION" || true
    done
    
    if [ ! -z "$NGS" ]; then
        log "Waiting for Node Groups to dissolve..."
        sleep 60
    fi

    # 3. Destroy Infrastructure
    log "Executing Terraform Destroy..."
    terraform destroy -auto-approve || true

    # 4. CLI Fallback (If TF failed, rip it out manually)
    if aws eks describe-cluster --name "$CLUSTER_NAME" --region "$REGION" > /dev/null 2>&1; then
        log "âš ï¸ Terraform failed to kill the cluster. Force-deleting via CLI..."
        aws eks delete-cluster --name "$CLUSTER_NAME" --region "$REGION" || true
        aws eks wait cluster-deleted --name "$CLUSTER_NAME" --region "$REGION" || true
    fi

    log "ðŸŒ‘ ZERO TRACE REMAINING."
}

# --- THE DEMO ---
function demo() {
    bootstrap
    log "ðŸŽ­ Launching Spectator..."
    cd "$TF_DIR"
    terraform init $INIT_ARGS -reconfigure > /dev/null
    NAME=$(terraform output -raw cluster_name)
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
spec:
  type: LoadBalancer
  selector: { ray.io/node-type: head }
  ports: [{ name: dash, port: 8265, targetPort: 8265 }]
EOF

    log "Waiting for DNS..."
    URL=""
    while [ -z "$URL" ]; do
        URL=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        sleep 10
    done
    log "ðŸš€ DASHBOARD: http://$URL:8265"
}

case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;;
esac