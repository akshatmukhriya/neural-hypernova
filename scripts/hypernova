#!/bin/bash
set -e

# --- ANCHOR PATHS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $1${COLOR_RESET}"; }

function ignite() {
    log "ðŸš€ Ignite Sequence: Neural Hypernova V1.1.1"

    # 1. Terraform
    cd "$TF_DIR"
    terraform init -reconfigure
    terraform apply -auto-approve

    # 2. Metadata
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    
    # 3. Kubeconfig
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repos
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo update

    # 5. Cilium (Fixed Template Error)
    log "Injecting Cilium 1.16.5 (eBPF Data Plane)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. Karpenter Injection
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait

    # CRITICAL: Wait for Karpenter CRDs and Webhooks to be live
    log "Waiting for Karpenter Webhooks..."
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 7. KubeRay
    log "Deploying KubeRay Operator..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2

    # 8. Manifests
    log "Applying AI Forge Workloads..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "âœ… NEURAL HYPERNOVA IS LIVE."
}

function demo() {
    log "ðŸ” Discovering Hypernova Infrastructure..."
    
    # 1. Self-Discovery (Crucial for GitHub Actions context)
    cd "$TF_DIR"
    terraform init -reconfigure > /dev/null
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    
    log "Syncing Kubeconfig for $CLUSTER_NAME..."
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 2. Trigger Workload
    log "ðŸŽ­ Injecting Distributed GPU Workload..."
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml" # Ensure base cluster is up
    
    # 3. Expose Dashboard
    log "ðŸŒ Provisioning Public Access to Ray Dashboard..."
    # We target the service created by the Ray Operator
    kubectl patch svc raycluster-hypernova-ray-head-svc -p '{"spec": {"type": "LoadBalancer"}}'

    # 4. Wait & Report
    log "â³ Waiting for NLB Propagation (Observation Mode)..."
    local ELB_DNS=""
    # Timeout after 2 minutes if NLB doesn't show up
    for i in {1..12}; do
        ELB_DNS=$(kubectl get svc raycluster-hypernova-ray-head-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        if [ ! -z "$ELB_DNS" ]; then
            break
        fi
        log "Still waiting for LoadBalancer DNS... ($((i*10))s)"
        sleep 10
    done

    if [ -z "$ELB_DNS" ]; then
        log "âš ï¸ NLB taking longer than expected. Check AWS Console for the DNS name."
    else
        echo -e "${COLOR_GREEN}"
        echo "----------------------------------------------------------------"
        echo "ðŸš€ NEURAL HYPERNOVA IS LIVE"
        echo "----------------------------------------------------------------"
        echo "ACCESS UI: http://$ELB_DNS:8265"
        echo "----------------------------------------------------------------"
        echo "OBSERVATION COMMANDS:"
        echo "1. Watch Nodes:  kubectl get nodes -l karpenter.sh/nodepool=gpu-forge"
        echo "2. Watch Pods:   kubectl get pods -n default"
        echo "----------------------------------------------------------------"
        echo -e "${COLOR_RESET}"
    fi
}

function nuke() {
    log "â˜¢ï¸ Scorched Earth Protocol..."
    cd "$TF_DIR"
    # Try to get metadata for CLI cleanup before destroying TF
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    
    # 1. Delete K8s Resources
    kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found || true
    
    # 2. Terraform Destroy
    terraform destroy -auto-approve || true
    
    # 3. CLI Clean up for orphaned ENIs
    log "Final Sweep of Security Groups..."
    SGS=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*$CLUSTER_NAME*" --query "SecurityGroups[*].GroupId" --output text)
    for sg in $SGS; do
        aws ec2 delete-security-group --group-id $sg || true
    done
    log "ðŸŒ‘ ZERO TRACE."
}

# --- THE COMMAND ROUTER (Ensure 'demo' is listed here!) ---
case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;; # THIS WAS LIKELY MISSING
    *) echo "Usage: $0 {ignite|nuke|demo}" ;;
esac
