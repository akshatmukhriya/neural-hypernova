#!/bin/bash
set -e

# --- PATHS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_RESET='\033[0m'
function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $1${COLOR_RESET}"; }

function ignite() {
    log "ðŸš€ Ignite Sequence: Neural Hypernova V1.1.1"

    # 1. Terraform
    cd "$TF_DIR"
    terraform init -reconfigure
    terraform apply -auto-approve

    # 2. Metadata
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    
    # 3. Kubeconfig
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repos
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo update

    # 5. Cilium (Fixed Template Error)
    log "Injecting Cilium 1.16.5 (eBPF Data Plane)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. Karpenter Injection
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait

    # CRITICAL: Wait for Karpenter CRDs and Webhooks to be live
    log "Waiting for Karpenter Webhooks..."
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 7. KubeRay
    log "Deploying KubeRay Operator..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2

    # 8. Manifests
    log "Applying AI Forge Workloads..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "âœ… NEURAL HYPERNOVA IS LIVE."
}

function demo() {
    log "ðŸŽ­ Triggering Distributed GPU Workload..."
    kubectl apply -f k8s/ai-forge/stress-test.yaml

    log "ðŸŒ Exposing Ray Dashboard via High-Performance NLB..."
    # We patch the Ray Head service to be a LoadBalancer
    kubectl patch svc raycluster-hypernova-ray-head-svc -p '{"spec": {"type": "LoadBalancer"}}'

    log "â³ Waiting for NLB to provision (this takes ~2 mins)..."
    # Wait for the external IP to be assigned
    local ELB_DNS=""
    while [ -z "$ELB_DNS" ]; do
        ELB_DNS=$(kubectl get svc raycluster-hypernova-ray-head-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        [ -z "$ELB_DNS" ] && sleep 10
    done

    echo -e "${COLOR_GREEN}"
    echo "----------------------------------------------------------------"
    echo "ðŸš€ NEURAL HYPERNOVA SPECTATOR MODE IS ACTIVE"
    echo "----------------------------------------------------------------"
    echo "RAY DASHBOARD URL: http://$ELB_DNS:8265"
    echo "----------------------------------------------------------------"
    echo "STATS TO SHOWCASE:"
    echo "1. SCALE: Run 'kubectl get nodes' to see the GPU nodes Karpenter spawned."
    echo "2. NETWORKING: Cilium eBPF is handling all inter-pod traffic at 100Gbps."
    echo "3. EFFICIENCY: This cluster costs \$0.00 when the RayJob finishes."
    echo "----------------------------------------------------------------"
    echo -e "${COLOR_RESET}"
}

function nuke() {
    log "â˜¢ï¸ Scorched Earth Protocol..."
    cd "$TF_DIR"
    # Try to get metadata for CLI cleanup before destroying TF
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    
    # 1. Delete K8s Resources
    kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found || true
    
    # 2. Terraform Destroy
    terraform destroy -auto-approve || true
    
    # 3. CLI Clean up for orphaned ENIs
    log "Final Sweep of Security Groups..."
    SGS=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*$CLUSTER_NAME*" --query "SecurityGroups[*].GroupId" --output text)
    for sg in $SGS; do
        aws ec2 delete-security-group --group-id $sg || true
    done
    log "ðŸŒ‘ ZERO TRACE."
}

case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
esac