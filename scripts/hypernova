#!/bin/bash
set -e

# --- ANCHORS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $(date +'%H:%M:%S') | $1${COLOR_RESET}"; }

function bootstrap() {
    log "ðŸ“¡ Detecting Identity..."
    RUNNER_ARN=$(aws sts get-caller-identity --query Arn --output text)
    ACCOUNT_ID=$(echo $RUNNER_ARN | cut -d: -f5)
    
    if [[ $RUNNER_ARN == *"assumed-role"* ]]; then
        ROLE_NAME=$(echo $RUNNER_ARN | cut -d/ -f2)
        CLEAN_ARN="arn:aws:iam::$ACCOUNT_ID:role/$ROLE_NAME"
    else
        CLEAN_ARN=$RUNNER_ARN
    fi

    BUCKET="hypernova-state-$ACCOUNT_ID"
    # Ensure bucket exists
    aws s3 mb "s3://$BUCKET" --region us-east-1 2>/dev/null || true
    
    INIT_OPTS=(-backend-config="bucket=$BUCKET")
    APPLY_OPTS=(-var="runner_arn=$CLEAN_ARN" -auto-approve)
}

function ignite() {
    bootstrap
    log "ðŸš€ Ignite Sequence: Neural Hypernova V1.2.7 (The Connectivity Guard)"
    cd "$TF_DIR"
    
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure
    terraform apply "${APPLY_OPTS[@]}"

    NAME=$(terraform output -raw cluster_name)
    VPC_ID=$(terraform output -raw vpc_id)
    ROLE_ARN=$(terraform output -raw lb_role_arn)

    log "ðŸ“¡ Synchronizing Kubeconfig for $NAME..."
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    # --- THE CONNECTIVITY GUARD (The Final Fix) ---
    log "â³ Phase 1: Waiting for Public API Endpoint to go Global..."
    local ATTEMPTS=0
    local MAX_ATTEMPTS=20
    # We poll 'kubectl version' because it tests the API connection without needing full RBAC yet
    until kubectl version --client=false --request-timeout='5s' > /dev/null 2>&1 || [ $ATTEMPTS -eq $MAX_ATTEMPTS ]; do
        log "API Endpoint unreachable (DNS/Propagation Lag)... (Attempt $((ATTEMPTS+1))/$MAX_ATTEMPTS)"
        sleep 20
        ATTEMPTS=$((ATTEMPTS+1))
        # Re-syncing kubeconfig can help refresh the cached endpoint IP
        aws eks update-kubeconfig --region us-east-1 --name "$NAME" > /dev/null
    done

    if [ $ATTEMPTS -eq $MAX_ATTEMPTS ]; then
        error_log "âŒ EKS Public API failed to stabilize. Check EKS Public Endpoint settings in AWS Console."
        exit 1
    fi
    log "âœ… Public API Handshake Verified."

    log "â³ Phase 2: Waiting for RBAC Propagation..."
    sleep 15 # Short pause for Access Entries to settle

    # --- CORE LAYERS ---
    log "ðŸ› ï¸ Registering Operational Repositories..."
    helm repo add cilium https://helm.cilium.io/
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add ray-project https://ray-project.github.io/kuberay-helm/
    helm repo update

    log "ðŸ’‰ Injecting Cilium eBPF (Schema-Compliant)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose=null \
      --set operator.replicas=1 \
      --wait

    log "ðŸ’‰ Injecting AWS LBC & Tools..."
    kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system \
      --set clusterName="$NAME" --set serviceAccount.create=true \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ROLE_ARN" \
      --set region=us-east-1 --set vpcId="$VPC_ID" --wait

    log "ðŸ’‰ Injecting Karpenter & KubeRay..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.1.1 -n karpenter --create-namespace --set settings.clusterName="$NAME" --wait
    helm upgrade --install kuberay-operator ray-project/kuberay-operator --version 1.2.2 --wait

    log "ðŸ—ï¸ Final Manifest Application..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"
    
    log "âœ… NEURAL HYPERNOVA IS LIVE."
}

function nuke() {
    bootstrap
    log "â˜¢ï¸ SCORCHED EARTH..."
    cd "$TF_DIR"
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure || true
    
    VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
    REGION="us-east-1"

    log "1. Removing K8s Services..."
    kubectl delete svc hypernova-dashboard-lb --ignore-not-found --timeout=30s || true
    
    log "2. Releasing Elastic IPs (Fixing 'mapped public address' deadlock)..."
    EIPS=$(aws ec2 describe-addresses --region $REGION --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text)
    for eip in $EIPS; do
        aws ec2 release-address --allocation-id "$eip" --region $REGION || true
    done

    log "3. Terraform Destroy..."
    terraform destroy "${APPLY_OPTS[@]}" || true
    
    log "4. Manual ENI Cleanup..."
    if [ ! -z "$VPC_ID" ] && [ "$VPC_ID" != "null" ]; then
        ENIS=$(aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text)
        for eni in $ENIS; do aws ec2 delete-network-interface --network-interface-id "$eni" || true; done
        aws eks delete-cluster --name neural-hypernova --region $REGION || true
    fi
    log "ðŸŒ‘ ZERO TRACE."
}

case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;;
esac