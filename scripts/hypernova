#!/bin/bash
set -e

# --- ANCHOR PATHS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $1${COLOR_RESET}"; }

function ignite() {
    log "üöÄ Ignite Sequence: Neural Hypernova V1.1.1"

    # 1. Terraform
    cd "$TF_DIR"
    terraform init -reconfigure
    terraform apply -auto-approve

    # 2. Metadata
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    
    # 3. Kubeconfig
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repos
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo update

    # 5. Cilium (Fixed Template Error)
    log "Injecting Cilium 1.16.5 (eBPF Data Plane)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. Karpenter Injection
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait

    # CRITICAL: Wait for Karpenter CRDs and Webhooks to be live
    log "Waiting for Karpenter Webhooks..."
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 7. KubeRay
    log "Deploying KubeRay Operator..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2

    log "Injecting NVIDIA GPU Discovery Plugin..."
    helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
    helm repo update
    helm upgrade --install nvidia-device-plugin nvdp/nvidia-device-plugin \
      --namespace kube-system \
      --set gfd.enabled=true \
      --wait

    # 8. Manifests
    log "Applying AI Forge Workloads..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "‚úÖ NEURAL HYPERNOVA IS LIVE."
}

function demo() {
    log "üîç Discovering Hypernova Infrastructure..."
    
    # 1. Self-Discovery & Context Recovery
    cd "$TF_DIR"
    terraform init -reconfigure > /dev/null
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 2. Inject Dashboard Proxy with Advanced Annotations
    log "üåê Provisioning High-Performance NLB (Layer 4)..."
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    # Ensures the NLB is public and uses the latest AWS Load Balancer Controller logic
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
    # Crucial: This tells AWS to open the security groups automatically
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
spec:
  type: LoadBalancer
  selector:
    ray.io/node-type: head
    ray.io/cluster: hypernova-ray
  ports:
  - name: dashboard
    protocol: TCP
    port: 8265
    targetPort: 8265
EOF

    # 3. Phase 1: AWS Resource Assignment
    log "‚è≥ Phase 1: Waiting for DNS Assignment..."
    local ELB_DNS=""
    while [ -z "$ELB_DNS" ]; do
        ELB_DNS=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        [ -z "$ELB_DNS" ] && sleep 10
    done
    log "DNS Identity Assigned: $ELB_DNS"

    # 4. Phase 2: DNS Propagation Check (Fixes NXDOMAIN)
    log "‚è≥ Phase 2: Waiting for DNS Propagation (Global Resolution)..."
    local RESOLVED="false"
    local DNS_ATTEMPTS=0
    while [ "$RESOLVED" != "true" ] && [ $DNS_ATTEMPTS -lt 20 ]; do
        if nslookup "$ELB_DNS" > /dev/null 2>&1; then
            RESOLVED="true"
            log "‚úÖ DNS Propagation successful."
        else
            log "Still waiting for DNS propagation... (NXDOMAIN Mitigation) ($((DNS_ATTEMPTS*15))s)"
            sleep 15
            DNS_ATTEMPTS=$((DNS_ATTEMPTS+1))
        fi
    done

    # 5. Phase 3: Security Group & Port Verification
    log "‚è≥ Phase 3: Final Port Warm-up & Security Sync..."
    # We explicitly verify the SG has the port open in the background via Terraform
    # or manual AWS CLI if Terraform isn't managing this specific rule.
    sleep 30

    echo -e "${COLOR_GREEN}"
    echo "----------------------------------------------------------------"
    echo "üöÄ NEURAL HYPERNOVA: SOVEREIGN FORGE ONLINE"
    echo "----------------------------------------------------------------"
    echo "RAY DASHBOARD: http://$ELB_DNS:8265"
    echo "----------------------------------------------------------------"
    echo "ARCHITECTURAL INTEGRITY:"
    echo "1. Entry: AWS NLB (Active & Resolved)"
    echo "2. Network: Cilium eBPF Data-Plane Verified"
    echo "3. Scale: Karpenter GPU NodePool Ready"
    echo "----------------------------------------------------------------"
    echo -e "${COLOR_RESET}"
}

function nuke() {
    log "‚ò¢Ô∏è Scorched Earth Protocol..."
    cd "$TF_DIR"
    # Try to get metadata for CLI cleanup before destroying TF
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    
    # 1. Delete K8s Resources
    kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found || true
    
    # 2. Terraform Destroy
    terraform destroy -auto-approve || true
    
    # 3. CLI Clean up for orphaned ENIs
    log "Final Sweep of Security Groups..."
    SGS=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*$CLUSTER_NAME*" --query "SecurityGroups[*].GroupId" --output text)
    for sg in $SGS; do
        aws ec2 delete-security-group --group-id $sg || true
    done
    log "üåë ZERO TRACE."
}

# --- THE COMMAND ROUTER (Ensure 'demo' is listed here!) ---
case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;; # THIS WAS LIKELY MISSING
    *) echo "Usage: $0 {ignite|nuke|demo}" ;;
esac
