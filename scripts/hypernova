#!/bin/bash
set -e

# --- ANCHORS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA] $1${COLOR_RESET}"; }

function bootstrap() {
    log "ğŸ“¡ Detecting Identity..."
    RUNNER_ARN=$(aws sts get-caller-identity --query Arn --output text)
    ACCOUNT_ID=$(echo $RUNNER_ARN | cut -d: -f5)
    [[ $RUNNER_ARN == *"assumed-role"* ]] && CLEAN_ARN="arn:aws:iam::$ACCOUNT_ID:role/$(echo $RUNNER_ARN | cut -d/ -f2)" || CLEAN_ARN=$RUNNER_ARN
    BUCKET="hypernova-state-$ACCOUNT_ID"
    aws s3 mb "s3://$BUCKET" --region us-east-1 2>/dev/null || true
    INIT_OPTS=(-backend-config="bucket=$BUCKET")
}

function ignite() {
    bootstrap
    log "ğŸš€ Ignite Sequence: V43.0.0 (Identity Lockdown)"
    
    # Quota Recovery
    for eip in $(aws ec2 describe-addresses --query "Addresses[?AssociationId==null].AllocationId" --output text); do
      aws ec2 release-address --allocation-id $eip || true
    done

    cd "$TF_DIR"
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure
    terraform apply -auto-approve

    NAME=$(terraform output -raw cluster_name)
    RAND_ID=$(terraform output -raw random_id)
    QUEUE_NAME=$(terraform output -raw interruption_queue_name)
    CONTROLLER_ROLE=$(terraform output -raw karpenter_controller_role)
    
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    log "ğŸ’‰ Phase 1: Cilium eBPF..."
    helm repo add cilium https://helm.cilium.io/
    helm repo update
    helm upgrade --install cilium cilium/cilium --version 1.16.5 -n kube-system \
      --set kubeProxyReplacement=true --set-string debug.verbose="" --wait

    log "ğŸ’‰ Phase 2: Karpenter..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.1.1 \
      -n karpenter --create-namespace \
      --set "serviceAccount.annotations.eks\.amazonaws\.com/role-arn=$CONTROLLER_ROLE" \
      --set settings.clusterName="$NAME" \
      --set settings.interruptionQueue="$QUEUE_NAME" --wait

    log "ğŸ’‰ Phase 3: KubeRay..."
    helm repo add ray https://ray-project.github.io/kuberay-helm/
    helm repo update
    helm upgrade --install kuberay ray/kuberay-operator --version 1.2.2 --wait

    log "ğŸ—ï¸ Injecting Discovery Tag: hypernova-$RAND_ID"
    sed "s/DISCOVERY_TAG_PLACEHOLDER/hypernova-$RAND_ID/g" "$K8S_DIR/core/karpenter-gpu-pool.yaml" > "$K8S_DIR/core/active-pool.yaml"

    kubectl apply -f "$K8S_DIR/core/active-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"
    log "âœ… SYSTEM ARMED."
}

function demo() {
    bootstrap
    log "ğŸ­ Launching Spectator Protocol (Cloud-Native Bypass)..."
    cd "$TF_DIR"
    terraform init "${INIT_OPTS[@]}" -reconfigure > /dev/null
    
    NAME=$(terraform output -raw cluster_name)
    VPC_ID=$(terraform output -raw vpc_id)
    RAND_ID=$(terraform output -raw random_id)
    SUBNETS=$(terraform output -json public_subnets | jq -r '.[]' | paste -sd " " -)
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    log "ğŸŒ Step 1: Deploying Dashboard NodePort..."
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-nodeport
spec:
  type: NodePort
  selector: { ray.io/node-type: head, ray.io/cluster: hypernova-ray }
  ports: [{ nodePort: 30265, port: 8265, targetPort: 8265 }]
EOF

    log "â³ Phase 0: Waiting for Ray Pod..."
    until kubectl get pod -l ray.io/node-type=head | grep "1/1" > /dev/null 2>&1; do
        log "Pod is warming up..."
        sleep 15
    done

    log "ğŸŒ Step 2: Building Hardware NLB..."
    LB_NAME="hyp-lb-$RAND_ID"
    LB_ARN=$(aws elbv2 create-load-balancer --name "$LB_NAME" --subnets $SUBNETS --type network --query "LoadBalancers[0].LoadBalancerArn" --output text)
    TG_ARN=$(aws elbv2 create-target-group --name "hyp-tg-$RAND_ID" --protocol TCP --port 30265 --vpc-id "$VPC_ID" --target-type instance --query "TargetGroups[0].TargetGroupArn" --output text)
    
    INSTANCES=$(aws ec2 describe-instances --filters "Name=tag:kubernetes.io/cluster/$NAME,Values=owned" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].InstanceId" --output text)
    for ins in $INSTANCES; do aws elbv2 register-targets --target-group-arn "$TG_ARN" --targets Id="$ins" || true; done
    aws elbv2 create-listener --load-balancer-arn "$LB_ARN" --protocol TCP --port 8265 --default-actions Type=forward,TargetGroupArn="$TG_ARN" > /dev/null

    URL=$(aws elbv2 describe-load-balancers --load-balancer-arns "$LB_ARN" --query "LoadBalancers[0].DNSName" --output text)
    echo -e "${COLOR_GREEN}ğŸš€ DASHBOARD: http://$URL:8265${COLOR_RESET}"
}

function nuke() {
    bootstrap
    log "â˜¢ï¸ NUCLEAR RESET..."
    cd "$TF_DIR"
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure || true
    
    VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
    RAND_ID=$(terraform output -raw random_id 2>/dev/null || echo "final")
    NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "hypernova")

    log "1. Purging Hardware..."
    LB_ARNS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'hyp-lb')].LoadBalancerArn" --output text)
    for lb in $LB_ARNS; do aws elbv2 delete-load-balancer --load-balancer-arn "$lb" || true; done

    log "2. Erasing Cluster..."
    aws eks delete-cluster --name "$NAME" --region us-east-1 2>/dev/null || true
    for eip in $(aws ec2 describe-addresses --region us-east-1 --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text); do
      aws ec2 release-address --allocation-id "$eip" --region us-east-1 || true
    done
    terraform destroy -auto-approve || true
    log "ğŸŒ‘ CLEAN."
}

case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;;
esac