#!/bin/bash
set -e

# --- ARCHITECTURAL ANCHORS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

# --- ELITE VISUALS ---
COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RED='\033[0;31m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA-LOG] $(date +'%H:%M:%S') | $1${COLOR_RESET}"; }
function error_log() { echo -e "${COLOR_RED}[HYPERNOVA-ERROR] $1${COLOR_RESET}"; }


# --- THE BOOTSTRAP ENGINE ---
function bootstrap_foundation() {
    log "üì° Detecting AWS Environment..."
    ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    BUCKET_NAME="hypernova-state-$ACCOUNT_ID" # Globally unique bucket name
    REGION="us-east-1"

    log "üî≠ Verifying State Persistence: $BUCKET_NAME"
    if ! aws s3 ls "s3://$BUCKET_NAME" > /dev/null 2>&1; then
        log "üèóÔ∏è Foundation not found. Creating S3 Sovereign State Bucket..."
        aws s3 mb "s3://$BUCKET_NAME" --region $REGION
        aws s3api put-bucket-versioning --bucket "$BUCKET_NAME" --versioning-configuration Status=Enabled
    fi

    # THE MAGIC: Dynamically pass the bucket name to Terraform
    # This overrides whatever is in backend.tf
    export TF_CLI_ARGS_init="-backend-config=\"bucket=$BUCKET_NAME\""
    log "‚úÖ Foundation Ready. State locked to $BUCKET_NAME"
}


# --- THE IGNITE PROTOCOL (Zero to Sovereign) ---
function ignite() {
    cd "$TF_DIR"
    # THE RUTHLESS KILL: Remove the ghost file if it exists in the repo
    [ -f "backend.tf" ] && rm -f "backend.tf" && log "üî• Redundant backend.tf purged."
    
    bootstrap_foundation
    
    log "Initializing Terraform with Persistent State..."
    # We pass the dynamic bucket name directly into the init command
    terraform init $INIT_ARGS -reconfigure
    terraform apply -auto-approve

    # 2. Metadata Extraction
    CLUSTER_NAME=$(terraform output -raw cluster_name)
    REGION=$(terraform output -raw region)
    VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --query "cluster.resourcesVpcConfig.vpcId" --output text)
    log "Cluster DNA Extracted: $CLUSTER_NAME in $REGION (VPC: $VPC_ID)"

    # 3. Kubeconfig Synchronization
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 4. Helm Repository Registry
    log "Registering High-Scale Repositories..."
    helm repo add cilium https://helm.cilium.io/
    helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
    helm repo update

    # 5. Data Plane Injection (Cilium eBPF)
    log "Injecting Cilium 1.16.5 (Kernel-Level Networking)..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 \
      --namespace kube-system \
      --set kubeProxyReplacement=true \
      --set debug.enabled=false \
      --set debug.verbose="null" \
      --wait || true

    # 6. AWS Load Balancer Controller (The Connectivity Bridge)
    log "Deploying AWS Load Balancer Controller with IRSA..."
    ROLE_ARN=$(cd "$TF_DIR" && terraform output -raw lb_controller_role_arn)

    # Create the ServiceAccount manually to ensure the annotation is perfect
    kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | \
    kubectl patch --local=true -f - -p "{\"metadata\":{\"annotations\":{\"eks.amazonaws.com/role-arn\":\"$ROLE_ARN\"}}}" --dry-run=client -o yaml | \
    kubectl apply -f -

    # Install the Controller, telling it to use the existing ServiceAccount
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
      --namespace kube-system \
      --set clusterName="$CLUSTER_NAME" \
      --set serviceAccount.create=false \
      --set serviceAccount.name=aws-load-balancer-controller \
      --set region="$REGION" \
      --set vpcId="$VPC_ID" \
      --wait

    # 7. Karpenter (Just-In-Time GPU Scaling)
    log "Injecting Karpenter v1.1.1..."
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
      --version 1.1.1 \
      --namespace karpenter --create-namespace \
      --set settings.clusterName="$CLUSTER_NAME" \
      --wait
    
    kubectl wait --for=condition=Available --timeout=60s deployment/karpenter -n karpenter

    # 8. AI Forge Core (KubeRay + NVIDIA)
    log "Deploying AI Orchestration (KubeRay & NVIDIA Plugin)..."
    helm upgrade --install kuberay-operator kuberay/kuberay-operator --version 1.2.2
    helm upgrade --install nvidia-device-plugin nvdp/nvidia-device-plugin \
      --namespace kube-system \
      --set gfd.enabled=true \
      --wait

    # 9. Workload Activation
    log "Applying Neural Hypernova Manifests..."
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    log "‚úÖ NEURAL HYPERNOVA IS ARMED AND OPERATIONAL."
}

# --- THE DEMO PROTOCOL (Public Visibility) ---
function demo() {
    log "üîç Discovering Hypernova Context..."
    
    cd "$TF_DIR"

    # Kill the redundant backend file if it exists to prevent HCL conflict
    [ -f "backend.tf" ] && rm -f "backend.tf" && log "üî• Ghost backend.tf purged."

    terraform init -reconfigure > /dev/null
    CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
    REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")
    aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME"

    # 1. Create Public Proxy (Avoiding Headless Service Collision)
    log "üåê Provisioning Industrial NLB via AWS LBC..."
    kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
spec:
  type: LoadBalancer
  selector:
    ray.io/node-type: head
  ports:
  - name: dashboard
    protocol: TCP
    port: 8265
    targetPort: 8265
EOF

    # 3. Phase 1: DNS Assignment with Diagnostic Timeout
    log "‚è≥ Phase 1: Waiting for DNS Assignment (60s Timeout)..."
    local ELB_DNS=""
    local COUNTER=0
    while [ -z "$ELB_DNS" ] && [ $COUNTER -lt 6 ]; do
        ELB_DNS=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        if [ -z "$ELB_DNS" ]; then
            log "Still waiting for DNS... ($((COUNTER*10))s)"
            sleep 10
            COUNTER=$((COUNTER+1))
        fi
    done

    if [ -z "$ELB_DNS" ]; then
        error_log "‚ùå DNS Assignment Timed Out. Checking Controller Health..."
        echo "--- AWS LOAD BALANCER CONTROLLER LOGS ---"
        kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20
        echo "--- SERVICE EVENTS ---"
        kubectl describe svc hypernova-dashboard-lb
        exit 1
    fi

    # 3. Global DNS Resolution Guard
    log "‚è≥ Phase 2: Waiting for Global DNS Propagation (NXDOMAIN Guard)..."
    local RESOLVED="false"
    local ATTEMPTS=0
    while [ "$RESOLVED" != "true" ] && [ $ATTEMPTS -lt 20 ]; do
        if nslookup "$ELB_DNS" > /dev/null 2>&1; then
            RESOLVED="true"
            log "‚úÖ DNS Resolution Verified."
        else
            log "Still propagating... ($((ATTEMPTS*15))s)"
            sleep 15
            ATTEMPTS=$((ATTEMPTS+1))
        fi
    done

    # 4. Connectivity Finalization
    log "‚è≥ Phase 3: NLB Target Warming..."
    sleep 30

    echo -e "${COLOR_GREEN}----------------------------------------------------------------"
    echo "üöÄ NEURAL HYPERNOVA: SPECTATOR MODE ONLINE"
    echo "----------------------------------------------------------------"
    echo "URL: http://$ELB_DNS:8265"
    echo "----------------------------------------------------------------"
    echo "ARCHITECTURAL VALIDATION:"
    echo "1. Entry: AWS NLB v2 (Active)"
    echo "2. Networking: Cilium eBPF (Bypassing Kube-Proxy)"
    echo "3. Compute: Karpenter JIT NodePool (Ready)"
    echo "----------------------------------------------------------------${COLOR_RESET}"
}

# --- THE NUKE PROTOCOL (Total Erasure) ---
function nuke() {
    cd "$TF_DIR"
    # THE RUTHLESS KILL: Remove the ghost file
    [ -f "backend.tf" ] && rm -f "backend.tf" && log "üî• Redundant backend.tf purged."
    
    bootstrap_foundation
    
    log "Refreshing State Context..."
    terraform init $INIT_ARGS -reconfigure || true

# 2. Extract Metadata with CLI Fallback
CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "neural-hypernova")
REGION=$(terraform output -raw region 2>/dev/null || echo "us-east-1")

# Discovery Fallback: If Terraform output fails, find VPC by Tag
VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || \
        aws ec2 describe-vpcs --filters "Name=tag:Name,Values=hypernova-vpc" --query "Vpcs[0].VpcId" --output text --region $REGION || echo "")

log "Targeting VPC: $VPC_ID"

# 3. Sync Context
aws eks update-kubeconfig --region "$REGION" --name "$CLUSTER_NAME" 2>/dev/null || true

# 4. K8s Resource Purge
log "Deleting K8s Resources..."
kubectl delete -f "$K8S_DIR/ai-forge/ray-cluster.yaml" --ignore-not-found --timeout=30s || true
kubectl delete svc hypernova-dashboard-lb --ignore-not-found --timeout=30s || true

log "‚åõ Waiting 60s for AWS LoadBalancer detachment..."
sleep 60

# 5. Terraform Destruction
log "Executing Terraform Destroy..."
terraform destroy -auto-approve || true

# 6. THE JANITOR SWEEP (CLI-Powered Brute Force)
if [ ! -z "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
    log "üßπ Hunting Orphaned Resources in VPC: $VPC_ID"
    
    # A. Kill all ENIs in this VPC
    ENIS=$(aws ec2 describe-network-interfaces --filters "Name=vpc-id,Values=$VPC_ID" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text)
    for eni in $ENIS; do
        log "Force-deleting ENI: $eni"
        aws ec2 delete-network-interface --network-interface-id "$eni" || true
    done

    # B. Kill all Security Groups in this VPC (except default)
    SGS=$(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query "SecurityGroups[?GroupName!='default'].GroupId" --output text)
    for sg in $SGS; do
        log "Force-deleting SG: $sg"
        aws ec2 delete-security-group --group-id "$sg" || true
    done
    
    # C. Final Cluster Deletion if it still exists
    aws eks delete-cluster --name "$CLUSTER_NAME" --region "$REGION" 2>/dev/null || true
fi

log "üåë ZERO TRACE REMAINING. ACCOUNT IS CLEAN."
}

# --- ROUTER ---
case "$1" in
    ignite) ignite ;;
    demo) demo ;;
    nuke) nuke ;;
    *) echo "Usage: $0 {ignite|demo|nuke}" ;;
esac