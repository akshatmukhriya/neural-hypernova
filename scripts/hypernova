#!/bin/bash
set -e

# --- ANCHORS ---
ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
TF_DIR="$ROOT_DIR/infra/terraform"
K8S_DIR="$ROOT_DIR/k8s"

COLOR_CYAN='\033[0;36m'
COLOR_GREEN='\033[0;32m'
COLOR_RESET='\033[0m'

function log() { echo -e "${COLOR_CYAN}[HYPERNOVA] $1${COLOR_RESET}"; }

function bootstrap() {
    log "ğŸ“¡ Detecting Identity..."
    RUNNER_ARN=$(aws sts get-caller-identity --query Arn --output text)
    ACCOUNT_ID=$(echo $RUNNER_ARN | cut -d: -f5)
    BUCKET="hypernova-state-$ACCOUNT_ID"
    aws s3 mb "s3://$BUCKET" --region us-east-1 2>/dev/null || true
    INIT_OPTS=(-backend-config="bucket=$BUCKET")
}

function ignite() {
    bootstrap
    log "ğŸš€ Ignite Sequence: V16.0.0 (Compute-First Protocol)"
    cd "$TF_DIR"
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure
    terraform apply -auto-approve

    NAME=$(terraform output -raw cluster_name)
    VPC_ID=$(terraform output -raw vpc_id)
    ROLE_ARN=$(terraform output -raw lb_role_arn)
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    # --- THE LOBOTOMY: Wipe any ghost webhooks from failed runs ---
    log "ğŸ§¹ Purging Ghost Webhooks..."
    kubectl delete mutatingwebhookconfiguration --all || true
    kubectl delete validatingwebhookconfiguration --all || true

    log "ğŸ› ï¸ Updating Repos..."
    helm repo add cilium https://helm.cilium.io/
    helm repo add eks https://aws.github.io/eks-charts
    helm repo add ray https://ray-project.github.io/kuberay-helm/
    helm repo update

    log "ğŸ’‰ Phase 1: Cilium eBPF..."
    helm upgrade --install cilium cilium/cilium --version 1.16.5 -n kube-system \
      --set kubeProxyReplacement=true --set-string debug.verbose="" --wait=false

    # --- THE COMPUTE-FIRST BYPASS ---
    log "ğŸ’‰ Phase 2: Karpenter & KubeRay..."
    # We install these BEFORE the Load Balancer Controller. 
    # There is no webhook present yet, so they install with ZERO resistance.
    helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.1.1 \
      -n karpenter --create-namespace --set settings.clusterName="$NAME" --wait=false
    
    helm upgrade --install kuberay-operator ray/kuberay-operator --version 1.2.2 --wait=false

    log "ğŸ—ï¸ Applying Forge Manifests..."
    sleep 20
    kubectl apply -f "$K8S_DIR/core/karpenter-gpu-pool.yaml"
    kubectl apply -f "$K8S_DIR/ai-forge/ray-cluster.yaml"

    # --- FINAL LAYER: NETWORKING ---
    log "ğŸ’‰ Phase 3: AWS Load Balancer Controller..."
    kubectl apply -f https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml
    helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system \
      --set clusterName="$NAME" --set serviceAccount.create=true \
      --set serviceAccount.name=aws-load-balancer-controller \
      --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="$ROLE_ARN" \
      --set region=us-east-1 --set vpcId="$VPC_ID" \
      --set webhookConfig.failurePolicy=Ignore \
      --wait=false

    log "âœ… SYSTEM ARMED. Hypernova is now stable."
}

function demo() {
    bootstrap
    log "ğŸ­ Launching Spectator Protocol..."
    cd "$TF_DIR"
    
    # 1. Sync State (Fresh Runner needs to talk to S3)
    log "Syncing state with foundation..."
    terraform init "${INIT_OPTS[@]}" -reconfigure > /dev/null
    NAME=$(terraform output -raw cluster_name)
    aws eks update-kubeconfig --region us-east-1 --name "$NAME"

    # 2. Kill Webhooks (Ensure Zero-Resistance for the Service creation)
    log "âš”ï¸  Clearing Webhook Sentries..."
    kubectl delete mutatingwebhookconfiguration elbv2.k8s.aws-mutating-webhook-configuration --ignore-not-found
    
    log "ğŸŒ Exposing Dashboard via AWS NLB..."
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hypernova-dashboard-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
spec:
  type: LoadBalancer
  selector: { ray.io/node-type: head }
  ports: [{ name: dash, port: 8265, targetPort: 8265 }]
EOF

    log "â³ Phase 1: DNS Identification..."
    local URL=""
    while [ -z "$URL" ]; do
        URL=$(kubectl get svc hypernova-dashboard-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
        [ ! -z "$URL" ] && break
        log "Waiting for AWS to assign DNS name... (10s)"
        sleep 10
    done
    log "DNS Assigned: $URL"

    log "â³ Phase 2: Propagation Guard..."
    until nslookup "$URL" > /dev/null 2>&1; do
        log "Waiting for DNS propagation... (15s)"
        sleep 15
    done

    echo -e "${COLOR_GREEN}----------------------------------------------------------------"
    echo "ğŸš€ NEURAL HYPERNOVA LIVE: http://$URL:8265"
    echo "----------------------------------------------------------------${COLOR_RESET}"
}

function nuke() {
    bootstrap
    log "â˜¢ï¸ NUCLEAR RESET..."
    cd "$TF_DIR"
    rm -rf .terraform .terraform.lock.hcl
    terraform init "${INIT_OPTS[@]}" -reconfigure || true
    VPC_ID=$(terraform output -raw vpc_id 2>/dev/null || echo "")
    NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "hypernova")

    kubectl delete svc hypernova-dashboard-lb --ignore-not-found || true
    aws eks delete-cluster --name "$NAME" --region us-east-1 2>/dev/null || true
    for eip in $(aws ec2 describe-addresses --region us-east-1 --query "Addresses[?VpcId=='$VPC_ID'].AllocationId" --output text); do
      aws ec2 release-address --allocation-id "$eip" --region us-east-1 || true
    done
    terraform destroy -auto-approve || true
    log "ğŸŒ‘ CLEAN."
}

case "$1" in
    ignite) ignite ;;
    nuke) nuke ;;
    demo) demo ;;
esac